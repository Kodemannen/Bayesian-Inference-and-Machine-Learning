{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian statistics\n",
    "\n",
    "#### Derivation of Bayes theorem:\n",
    "Derivation:\n",
    "\n",
    "We have two parameters $A$ and $B$. For two given values we have $p(A,B)$ as the probability that both of those values are the true of A and B.\n",
    "\n",
    "We start with the intuitive statement $$p(A,B) = p(A|B)p(B).$$\n",
    "\n",
    "But since $p(A,B) = p(B,A)$ it must follow that\n",
    "$$p(A|B)p(B) = p(B|A)p(A),$$\n",
    "\n",
    "which leads to Bayes theorem\n",
    "\n",
    "$$p(A|B) = \\frac{p(B|A)p(A)}{p(B)},$$\n",
    "\n",
    "usually written as \n",
    "\n",
    "$$p(A|B)  \\propto p(B|A)p(A)$$\n",
    "\n",
    "with $p(B)$ as a normalization constant to make sure that $\\int_A p(B|A')p(A')dA' = 1$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian Linear Regression\n",
    "\n",
    "We have a dataset $D = \\{d_1, d_2, .., d_N\\}$ that are measurements of value $y$ that is a function of a parameter vector $\\vec{x}$. In other words $d_i = y(\\vec{x}_i | \\boldsymbol{\\theta})$.\n",
    "\n",
    "$D$ and $X=\\{\\vec{x}_1, \\vec{x}_2, .., \\vec{x}_N\\}$ are known, and we want to find the function $y$, meaning we need to find its parameters $\\boldsymbol{\\theta}$ (if the shape/form of $y$ is assumed, otherwise we'd need to find the shape as well). \n",
    "\n",
    "In regular regression, one is only interested in the value for $\\boldsymbol{\\theta}$ that maximizes the probability of getting the obtained data, i.e.\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}} = \\underset{\\boldsymbol{\\theta}}{\\text{argmax}} p(D|\\boldsymbol{\\theta})\n",
    "$$\n",
    "\n",
    "The factor $p(D|\\boldsymbol{\\theta})$ is called the _likelihood function_ and describes the probability of getting the data $D$ if the given hypothesis $\\boldsymbol{\\theta}$ is true. $\\hat{\\boldsymbol{\\theta}}$ is known as the MLE (maximum likelihood estimate).  But this is just a point estimate and gives no information about the robustness of the estimate, i.e. how much the probability changes by moving to other points that are close to $\\hat{\\boldsymbol{\\theta}}$ in parameter space.\n",
    "\n",
    "This is something we can get with Bayesian linear regression.\n",
    "\n",
    "\n",
    "Any parameter configuration $\\boldsymbol{\\theta}$ is a unique hypothesis for the model.\n",
    "For any given $\\boldsymbol{\\theta}$, we want to know the probability of that hypothesis being true from the data, described as\n",
    "\n",
    "$$\n",
    "p(\\boldsymbol{\\theta}|D).\n",
    "$$\n",
    "\n",
    "We can then use Bayes theorem to get\n",
    "$$ p(\\boldsymbol{\\theta}|D)  \\propto {p(D|\\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}.$$\n",
    "\n",
    "The factor $p(\\boldsymbol{\\theta})$ is called the __prior distribution__  for the hypothesis, meaning the probability estimate for hypothesis $\\boldsymbol{\\theta}$ being true prior to seeing the data. If we have the likelihood and the prior, then we can create $p(\\boldsymbol{\\theta}|D)$ which is known as the __posterior distribution__.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Ridge Regression on 1D Ising model data\n",
    "\n",
    "To get the shape of the posterior distribution, we need to specify the likelihood and the prior. This is of course problem dependent.\n",
    "\n",
    "#### Choosing the Likelihood\n",
    "It is common to make the assumption that the data is __iid__ (identically and independently distributed).\n",
    "\n",
    "The likelihood can then be modelled as \n",
    "$$\n",
    "p(D|\\boldsymbol{\\theta}) = p(d_1|\\boldsymbol{\\theta})p(d_2|\\boldsymbol{\\theta})..p(d_N|\\boldsymbol{\\theta})\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\begin{align}\n",
    "p(d_i|\\boldsymbol{\\theta}) & = \\mathcal{N}(\\vec{w}^T\\vec{x}_i, \\sigma^2) \\\\ \n",
    "                           & \\propto \\exp \\Big(-\\dfrac{1}{\\sigma^2} (d_i-\\vec{w}^T\\vec{x}_i)^2\\Big)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $\\boldsymbol{\\theta} = \\{\\vec{w}, \\sigma^2\\}$. \n",
    "The Gaussian is commonly used because this is the probability distribution with the highest entropy for iids. In other words, if the data is iid, the Gaussian is the _most probable way for the data to be distributed_. Here we assume that the noise variation $\\sigma^2$ does not change with $\\vec{x}$, which is not always a correct assumption.\n",
    "\n",
    "\n",
    "The full likelihood is then\n",
    "$$\n",
    "p(D|\\boldsymbol{\\theta}) \\propto \\exp \\Big(-\\sum_i^N \\dfrac{1}{\\sigma^2} (d_i-\\vec{w}^T\\vec{x}_i)^2\\Big)\n",
    "$$\n",
    "\n",
    "\n",
    "#### Choosing the Prior\n",
    "We need to decide a shape for our prior \n",
    "$$\n",
    "p(\\boldsymbol{\\theta}) = p(\\vec{w},\\sigma^2).\n",
    "$$\n",
    "\n",
    "But for the \n",
    "\n",
    "A common choice is the zero mean Gaussian. \n",
    "This gives a higher prior probaility to functions with small, even parameters, i.e. smoother / less complex functions. \n",
    "This in a way captures the ide of Occam's Razor that we should prefer the simplest hypothesis that explains the data (although other zero zentered, symmetric distributions would do this as well).\n",
    "\n",
    "It makes it easier mathematically to pick a Gaussian when the likelihood is Gaussian as well (called conjugate prior).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying phases of the Ising model with Bayesian regression\n",
    "\n",
    "\n",
    "Different way to formulate regression than usual:\n",
    "\n",
    "$ P(y|\\boldsymbol{x},\\boldsymbol{\\theta}) = \\mathcal{N}(y|\\boldsymbol{w}^T\\boldsymbol{x}, \\sigma^2)$\n",
    "\n",
    "$\\boldsymbol{\\theta}$ represents the set of parameters.\n",
    "\n",
    "In other words, for every point $y(x)$ on the regression line, there is a gaussian distribution of data with center (mean) on $y$ with variance $\\sigma^2$.\n",
    "\n",
    "\n",
    "\n",
    "### Bayesian formulation of Ridge regression: \n",
    "Gaussian likelihood (iid assumption)\n",
    "\n",
    "$$\n",
    "p(D|\\boldsymbol{\\theta}) = p(d_1|\\boldsymbol{\\theta})p(d_2|\\boldsymbol{\\theta})..p(d_N|\\boldsymbol{\\theta})\n",
    "$$\n",
    "where\n",
    "$$\n",
    "p(d_i|\\boldsymbol{\\theta}) = \\mathcal{N}(w^Tx_i, \\sigma^2)\n",
    "$$\n",
    "\n",
    "with a Gaussian prior\n",
    "\n",
    "$$\n",
    "p(w_j) = \\mathcal{}\n",
    "$$\n",
    "\n",
    "\n",
    "This is equivalent to regression with l2 regularization\n",
    "\n",
    "\n",
    "Here the datapoints d_i are energy values for the given ising state x_i.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Difference between Bayesian and classical approach to statistics:\n",
    "Classical:\n",
    "    Frequentist. Probability represents empirical ratio of event / possible events\n",
    "    The data is the stochastic elements, the parameter is fixed.\n",
    "    Suffers from a number of pathologies\n",
    "    \n",
    "Bayesian:\n",
    "    Probability represents OUR uncertainty.\n",
    "    The data isn't the stochastic element, but our parameter estimate is\n",
    "    Uses a prior, representing that one usually has prior information\n",
    "    or the prior can represent complete ignorance\n",
    "    \n",
    "   \n",
    " \n",
    "Various regularization schemes are mathematically equivalent to using certain priors\n",
    "\n",
    "\n",
    "Bayesian linear regression:\n",
    "    Full posterior p(w,sigma|D), meaning we get the knowledge of how vectors w' in the neighbourhood of w compares, and we also get the posterior over sigma^2 meaning we get to know how reliable the estimation is!\n",
    "    But can we not just get that from the empirical variance in the data?\n",
    "    \n",
    "    Can provide a certainty estimate, eg. credible interval or highest posterior density region. We will use the latter. \n",
    "        what is this when doing linear regression?\n",
    "        \n",
    "       \n",
    "    Gaussian prior is equivalent to L2 regularization in classical statistics\n",
    "    \n",
    "    Bayesian model selection for linreg?\n",
    "    \n",
    "    gives uncertainty estimates both for the w and y\n",
    "    \n",
    "\n",
    "\"In most statistical situations\n",
    "(excluding game theoretic ones), assuming nature is an adversary is not a reasonable assumption.\" \n",
    "\n",
    "\n",
    "\n",
    "Ha med forklaringen for hvordan bayesisk tenkning kan brukes til Ã¥ si f.eks. noe om utenomjordisk liv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 1D ising model\n",
    "\n",
    "We randomly generate $N$ states of the 1D ising model (meaning N 1D vectors consisting of -1s and 1s) and calculate their energies using the following Hamiltonian:\n",
    "$$\n",
    "H[\\vec{S^i}] = - J\\sum_{j=1}^LS_j^i S_{j+1}^i\n",
    "$$\n",
    "Where $S_j^i$ is the j'th element of the i'th state $\\vec{S^i}$. We set the value $J=1$.\n",
    "\n",
    "We will then try to see if we can re-extract this Hamiltonian from the data using Bayesian Linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producing 1D Ising data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.68175651 -1.78809425]\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "np.random.seed(12)\n",
    "\n",
    "\n",
    "import warnings\n",
    "# Comment this to turn on warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "### define Ising model aprams\n",
    "# system size\n",
    "L=40\n",
    "\n",
    "# create 10000 random Ising states\n",
    "states=np.random.choice([-1, 1], size=(2,L))\n",
    "\n",
    "def ising_energies(states):\n",
    "    \"\"\"\n",
    "    This function calculates the energies of the states in the nn Ising Hamiltonian\n",
    "    \"\"\"\n",
    "    L = states.shape[1]\n",
    "    J = np.zeros((L, L),)\n",
    "    for i in range(L): \n",
    "        J[i,(i+1)%L]=-1.0 # interaction between nearest-neighbors\n",
    "        \n",
    "    # compute energies\n",
    "    E = np.einsum('...i,ij,...j->...',states,J,states)\n",
    "\n",
    "    return E\n",
    "# calculate Ising energies\n",
    "energies=ising_energies(states)\n",
    "\n",
    "# Adding noise:\n",
    "noise_variance = 1\n",
    "energies += np.random.normal(0,scale=np.sqrt(noise_varianace), size=energies.shape)\n",
    "print(energies.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remapping data for regression\n",
    "\n",
    "We pretend that we're ignorant about the Hamiltonian used to generate the above data. That means that the values aren't the only unknowns, but the shape of it as well. So we need to consider the all-to-all Hamiltonian\n",
    "\n",
    "$$\n",
    "H_{model}[\\vec{S^i}] = - \\sum_{j=1}^L\\sum_{k=1}^L J_{j,k}S_j^iS_{k}^i\n",
    "$$\n",
    "\n",
    "We see that the actual Hamiltonian we used above is just a special case of this, with $J_{j,k} = \\delta_{j,k+1}$.\n",
    "\n",
    "\n",
    "\n",
    "Taking the outer product\n",
    "\n",
    "$\\vec{\\mathbf{x}} \\rightarrow \\phi(\\vec{\\mathbf{x}})=\\vec{\\mathbf{x}}\\otimes \\vec{\\mathbf{x}}$\n",
    "\n",
    "then we make the vector $\\phi(\\vec{\\mathbf{x}})$ one-dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_states = np.einsum('bi,bo->bio',states,states)\n",
    "new_states = new_states.reshape(new_states.shape[0],-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When $\\sigma^2$ is assumed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-61cff1844ed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mVN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m     \u001b[0mainv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sys import exit\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "n = new_states.shape[0]   # number of data\n",
    "D = new_states.shape[1]   # data dimension\n",
    "\n",
    "# Prior:\n",
    "variance = 1\n",
    "w0 = np.zeros(D)\n",
    "tau = 10 # 1 means unitary gaussian, determines the strength of the prior\n",
    "V0 = tau**2*np.identity(D)  # precision matrix of prior\n",
    "V0_inv = np.linalg.inv(V0)\n",
    "\n",
    "\n",
    "X = new_states # data matrix with data as rows\n",
    "y = energies\n",
    "\n",
    "\n",
    "VN_inv = V0_inv + np.dot(X.T,X) / variance\n",
    "VN = np.linalg.inv(VN_inv)\n",
    "\n",
    "wN = np.dot(np.dot(VN,V0_inv),w0) + np.dot(np.dot(VN,X.T),y) / variance\n",
    "t1 = time.time()-t0\n",
    "\n",
    "\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import time\n",
    "from sys import exit\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "n = new_states.shape[0]   # number of data\n",
    "D = new_states.shape[1]   # data dimension\n",
    "\n",
    "# Prior:\n",
    "variance = 1\n",
    "w0 = np.zeros(D)\n",
    "tau = 1 # 1 means unitary gaussian, determines the strength of the prior\n",
    "V0 = tau**2*np.identity(D)  # precision matrix of prior\n",
    "V0_inv = np.linalg.inv(V0)\n",
    "\n",
    "\n",
    "X = new_states # data matrix with data as rows\n",
    "y = energies\n",
    "\n",
    "\n",
    "VN_inv = V0_inv + np.dot(X.T,X) / variance\n",
    "VN = np.linalg.inv(VN_inv)\n",
    "\n",
    "wN = np.dot(np.dot(VN,V0_inv),w0) + np.dot(np.dot(VN,X.T),y) / variance\n",
    "t1 = time.time()-t0\n",
    "\n",
    "\n",
    "print(t1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(wN.reshape(40,40))\n",
    "plt.show()\n",
    "#plt.imshow(wN.reshape(40,40))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gir mening at den fordeler verdiene i w sÃ¥nn, fordi 0.5^2 + 0.5^2 er mindre enn 1^2 + 1^2 \n",
    "\n",
    "Det at Lasso er mer riktig er ikke fordi den henter det ut fra dataen, men pga. prioren.\n",
    "\n",
    "Men er variansen 0 her egentlig?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9998383177596152\n"
     ]
    }
   ],
   "source": [
    "test_states=np.random.choice([-1, 1], size=(300,L))\n",
    "# calculate Ising test energies\n",
    "test_energies=ising_energies(test_states)\n",
    "\n",
    "# remapping states:\n",
    "test_states = np.einsum('bi,bo->bio',test_states,test_states)\n",
    "test_states = test_states.reshape(test_states.shape[0],-1)\n",
    "\n",
    "predicted_energies = np.dot(test_states, wN)\n",
    "\n",
    "\n",
    "### R^2 - coefficient of determination\n",
    "y_true_avg = np.mean(test_energies)\n",
    "residuals = predicted_energies - test_energies\n",
    "u = np.dot(residuals,residuals)\n",
    "v = test_energies - y_true_avg\n",
    "v = np.dot(v,v)\n",
    "\n",
    "R_squared = 1 - u/v\n",
    "\n",
    "print(R_squared)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When $\\sigma^2$ is not assumed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 2d Ising model\n",
    "### Loading Ising model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "def read_t(t,root=\"/home/samknu/MyRepos/MLProjectIsingModel/data/IsingData/\"):\n",
    "    data = pickle.load(open(root+'Ising2DFM_reSample_L40_T=%.2f.pkl'%t,'rb'))\n",
    "    return np.unpackbits(data).astype(int).reshape(-1,1600)\n",
    "\n",
    "temperatures = np.arange(0.25, 4., step=0.25)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
